Trying to run Tower as is given in the example from (https://huggingface.co/Unbabel/TowerInstruct-7B-v0.2):

Repository Not Found for url: https://huggingface.co/Unbabel/TowerInstruct-v0.2/resolve/main/adapter_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ashait/syntactic_creativity/few-shot/test_tower.py", line 8, in <module>
    pipe = pipeline("text-generation", model="Unbabel/TowerInstruct-v0.2", torch_dtype=torch.bfloat16, device_map="auto")
  File "/home/ashait/data/conda/envs/ml_llm/lib/python3.10/site-packages/transformers/pipelines/__init__.py", line 739, in pipeline
    maybe_adapter_path = find_adapter_config_file(
  File "/home/ashait/data/conda/envs/ml_llm/lib/python3.10/site-packages/transformers/utils/peft_utils.py", line 87, in find_adapter_config_file
    adapter_cached_filename = cached_file(
  File "/home/ashait/data/conda/envs/ml_llm/lib/python3.10/site-packages/transformers/utils/hub.py", line 450, in cached_file
    raise EnvironmentError(
OSError: Unbabel/TowerInstruct-v0.2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`

<!-- running ContraDecode -->
/home/ashait/data/conda/envs/ml_llm/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.